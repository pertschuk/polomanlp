{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pertschuk/polomanlp/blob/master/Bert_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "PUDryxu0L7Nx",
        "colab_type": "code",
        "outputId": "87e02957-db14-4006-9ffd-c0f07da45d1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import collections\n",
        "import logging\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "from pytorch_pretrained_bert.tokenization import printable_text, whitespace_tokenize, BasicTokenizer, BertTokenizer\n",
        "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering\n",
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# STATIC VARS\n",
        "MAX_SEQ_LENGTH = 384\n",
        "DOC_STRIDE = 128\n",
        "MAX_QUERY_LENGTH = 64\n",
        "PREDICT_BATCH_SIZE = 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c1a68434e7c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprintable_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBasicTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertForQuestionAnswering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_pretrained_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_pretrained_bert'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "zgmjHlfLMIod",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TfHeVgvYL7N1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "RawResult = collections.namedtuple(\"RawResult\",\n",
        "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p8qFRa10L7N2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _get_best_indexes(logits, n_best_size):\n",
        "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    best_indexes = []\n",
        "    for i in range(len(index_and_score)):\n",
        "        if i >= n_best_size:\n",
        "            break\n",
        "        best_indexes.append(index_and_score[i][0])\n",
        "    return best_indexes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4OZnZ_fPL7N6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "    def __init__(self,\n",
        "                 unique_id,\n",
        "                 sample_index,\n",
        "                 doc_span_index,\n",
        "                 tokens,\n",
        "                 token_to_orig_map,\n",
        "                 token_is_max_context,\n",
        "                 input_ids,\n",
        "                 input_mask,\n",
        "                 segment_ids,\n",
        "                 start_position=None,\n",
        "                 end_position=None):\n",
        "        self.unique_id = unique_id\n",
        "        self.sample_index = sample_index\n",
        "        self.doc_span_index = doc_span_index\n",
        "        self.tokens = tokens\n",
        "        self.token_to_orig_map = token_to_orig_map\n",
        "        self.token_is_max_context = token_is_max_context\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.start_position = start_position\n",
        "        self.end_position = end_position"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hlMRRDDcL7N9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n",
        "    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
        "\n",
        "    # When we created the data, we kept track of the alignment between original\n",
        "    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
        "    # now `orig_text` contains the span of our original text corresponding to the\n",
        "    # span that we predicted.\n",
        "    #\n",
        "    # However, `orig_text` may contain extra characters that we don't want in\n",
        "    # our prediction.\n",
        "    #\n",
        "    # For example, let's say:\n",
        "    #   pred_text = steve smith\n",
        "    #   orig_text = Steve Smith's\n",
        "    #\n",
        "    # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
        "    #\n",
        "    # We don't want to return `pred_text` because it's already been normalized\n",
        "    # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
        "    # our tokenizer does additional normalization like stripping accent\n",
        "    # characters).\n",
        "    #\n",
        "    # What we really want to return is \"Steve Smith\".\n",
        "    #\n",
        "    # Therefore, we have to apply a semi-complicated alignment heruistic between\n",
        "    # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n",
        "    # can fail in certain cases in which case we just return `orig_text`.\n",
        "\n",
        "    def _strip_spaces(text):\n",
        "        ns_chars = []\n",
        "        ns_to_s_map = collections.OrderedDict()\n",
        "        for (i, c) in enumerate(text):\n",
        "            if c == \" \":\n",
        "                continue\n",
        "            ns_to_s_map[len(ns_chars)] = i\n",
        "            ns_chars.append(c)\n",
        "        ns_text = \"\".join(ns_chars)\n",
        "        return (ns_text, ns_to_s_map)\n",
        "\n",
        "    # We first tokenize `orig_text`, strip whitespace from the result\n",
        "    # and `pred_text`, and check if they are the same length. If they are\n",
        "    # NOT the same length, the heuristic has failed. If they are the same\n",
        "    # length, we assume the characters are one-to-one aligned.\n",
        "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "\n",
        "    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
        "\n",
        "    start_position = tok_text.find(pred_text)\n",
        "    if start_position == -1:\n",
        "        if verbose_logging:\n",
        "            logger.info(\n",
        "                \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
        "        return orig_text\n",
        "    end_position = start_position + len(pred_text) - 1\n",
        "\n",
        "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
        "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
        "\n",
        "    if len(orig_ns_text) != len(tok_ns_text):\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
        "                            orig_ns_text, tok_ns_text)\n",
        "        return orig_text\n",
        "\n",
        "    # We then project the characters in `pred_text` back to `orig_text` using\n",
        "    # the character-to-character alignment.\n",
        "    tok_s_to_ns_map = {}\n",
        "    for (i, tok_index) in tok_ns_to_s_map.items():\n",
        "        tok_s_to_ns_map[tok_index] = i\n",
        "\n",
        "    orig_start_position = None\n",
        "    if start_position in tok_s_to_ns_map:\n",
        "        ns_start_position = tok_s_to_ns_map[start_position]\n",
        "        if ns_start_position in orig_ns_to_s_map:\n",
        "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
        "\n",
        "    if orig_start_position is None:\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Couldn't map start position\")\n",
        "        return orig_text\n",
        "\n",
        "    orig_end_position = None\n",
        "    if end_position in tok_s_to_ns_map:\n",
        "        ns_end_position = tok_s_to_ns_map[end_position]\n",
        "        if ns_end_position in orig_ns_to_s_map:\n",
        "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
        "\n",
        "    if orig_end_position is None:\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Couldn't map end position\")\n",
        "        return orig_text\n",
        "\n",
        "    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
        "    return output_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZntKvgNL7OA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _compute_softmax(scores):\n",
        "    \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
        "    if not scores:\n",
        "        return []\n",
        "\n",
        "    max_score = None\n",
        "    for score in scores:\n",
        "        if max_score is None or score > max_score:\n",
        "            max_score = score\n",
        "\n",
        "    exp_scores = []\n",
        "    total_sum = 0.0\n",
        "    for score in scores:\n",
        "        x = math.exp(score - max_score)\n",
        "        exp_scores.append(x)\n",
        "        total_sum += x\n",
        "\n",
        "    probs = []\n",
        "    for score in exp_scores:\n",
        "        probs.append(score / total_sum)\n",
        "    return probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HWPdzwToL7OD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
        "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
        "\n",
        "    # Because of the sliding window approach taken to scoring documents, a single\n",
        "    # token can appear in multiple documents. E.g.\n",
        "    #  Doc: the man went to the store and bought a gallon of milk\n",
        "    #  Span A: the man went to the\n",
        "    #  Span B: to the store and bought\n",
        "    #  Span C: and bought a gallon of\n",
        "    #  ...\n",
        "    #\n",
        "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
        "    # want to consider the score with \"maximum context\", which we define as\n",
        "    # the *minimum* of its left and right context (the *sum* of left and\n",
        "    # right context will always be the same, of course).\n",
        "    #\n",
        "    # In the example the maximum context for 'bought' would be span C since\n",
        "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
        "    # and 0 right context.\n",
        "    best_score = None\n",
        "    best_span_index = None\n",
        "    for (span_index, doc_span) in enumerate(doc_spans):\n",
        "        end = doc_span.start + doc_span.length - 1\n",
        "        if position < doc_span.start:\n",
        "            continue\n",
        "        if position > end:\n",
        "            continue\n",
        "        num_left_context = position - doc_span.start\n",
        "        num_right_context = end - position\n",
        "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_span_index = span_index\n",
        "\n",
        "    return cur_span_index == best_span_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9tcjLY8lL7OG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def write_predictions(all_examples, all_features, all_results,\n",
        "                      output_prediction_file):\n",
        "    \"\"\"Write final predictions to the json file.\"\"\"\n",
        "    max_answer_length = 500\n",
        "    do_lower_case = True\n",
        "    n_best_size = 5\n",
        "    logger.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
        "\n",
        "    example_index_to_features = collections.defaultdict(list)\n",
        "    for feature in all_features:\n",
        "        example_index_to_features[feature.sample_index].append(feature)\n",
        "\n",
        "    unique_id_to_result = {}\n",
        "    for result in all_results:\n",
        "        unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"PrelimPrediction\",\n",
        "        [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "    all_predictions = collections.OrderedDict()\n",
        "    for (example_index, example) in enumerate(all_examples):\n",
        "        features = example_index_to_features[example_index]\n",
        "\n",
        "        prelim_predictions = []\n",
        "        for (feature_index, feature) in enumerate(features):\n",
        "            result = unique_id_to_result[feature.unique_id]\n",
        "\n",
        "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # We could hypothetically create invalid predictions, e.g., predict\n",
        "                    # that the start of the span is in the question. We throw out all\n",
        "                    # invalid predictions.\n",
        "                    if start_index >= len(feature.tokens):\n",
        "                        continue\n",
        "                    if end_index >= len(feature.tokens):\n",
        "                        continue\n",
        "                    if start_index not in feature.token_to_orig_map:\n",
        "                        continue\n",
        "                    if end_index not in feature.token_to_orig_map:\n",
        "                        continue\n",
        "                    if not feature.token_is_max_context.get(start_index, False):\n",
        "                        continue\n",
        "                    if end_index < start_index:\n",
        "                        continue\n",
        "                    length = end_index - start_index + 1\n",
        "                    if length > max_answer_length:\n",
        "                        continue\n",
        "                    prelim_predictions.append(\n",
        "                        _PrelimPrediction(\n",
        "                            feature_index=feature_index,\n",
        "                            start_index=start_index,\n",
        "                            end_index=end_index,\n",
        "                            start_logit=result.start_logits[start_index],\n",
        "                            end_logit=result.end_logits[end_index]))\n",
        "\n",
        "        prelim_predictions = sorted(\n",
        "            prelim_predictions,\n",
        "            key=lambda x: (x.start_logit + x.end_logit),\n",
        "            reverse=True)\n",
        "\n",
        "        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "        seen_predictions = {}\n",
        "        nbest = []\n",
        "        for pred in prelim_predictions:\n",
        "            if len(nbest) >= n_best_size:\n",
        "                break\n",
        "            feature = features[pred.feature_index]\n",
        "\n",
        "            tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
        "            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "            dov_tokens = example['context']\n",
        "            orig_tokens = dov_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
        "            tok_text = \" \".join(tok_tokens)\n",
        "\n",
        "            # De-tokenize WordPieces that have been split off.\n",
        "            tok_text = tok_text.replace(\" ##\", \"\")\n",
        "            tok_text = tok_text.replace(\"##\", \"\")\n",
        "\n",
        "            # Clean whitespace\n",
        "            tok_text = tok_text.strip()\n",
        "            tok_text = \" \".join(tok_text.split())\n",
        "            orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "            final_text = get_final_text(tok_text, orig_text, do_lower_case)\n",
        "            if final_text in seen_predictions:\n",
        "                continue\n",
        "\n",
        "            seen_predictions[final_text] = True\n",
        "            nbest.append(\n",
        "                _NbestPrediction(\n",
        "                    text=final_text,\n",
        "                    start_logit=pred.start_logit,\n",
        "                    end_logit=pred.end_logit))\n",
        "\n",
        "        # In very rare edge cases we could have no valid predictions. So we\n",
        "        # just create a nonce prediction in this case to avoid failure.\n",
        "        if not nbest:\n",
        "            nbest.append(\n",
        "                _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "        assert len(nbest) >= 1\n",
        "\n",
        "        total_scores = []\n",
        "        for entry in nbest:\n",
        "            total_scores.append(entry.start_logit + entry.end_logit)\n",
        "\n",
        "        probs = _compute_softmax(total_scores)\n",
        "\n",
        "        nbest_json = []\n",
        "        for (i, entry) in enumerate(nbest):\n",
        "            output = collections.OrderedDict()\n",
        "            output[\"text\"] = entry.text\n",
        "            output[\"probability\"] = probs[i]\n",
        "            output[\"start_logit\"] = entry.start_logit\n",
        "            output[\"end_logit\"] = entry.end_logit\n",
        "            nbest_json.append(output)\n",
        "\n",
        "        assert len(nbest_json) >= 1\n",
        "\n",
        "        all_predictions[example['question']] = nbest_json[0][\"text\"]\n",
        "\n",
        "    with open(output_prediction_file, \"w\") as writer:\n",
        "        writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
        "        \n",
        "    return nbest_json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aueKsoALL7OJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# samples: {\"context\", \"question\"}\n",
        "def convert_samples_to_features(samples, tokenizer, max_seq_length,\n",
        "                                 doc_stride, max_query_length, is_training):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    unique_id = 1000000000\n",
        "\n",
        "    features = []\n",
        "    for (sample_index, sample) in enumerate(samples):\n",
        "        # QUESTION TEXT\n",
        "        query_tokens = tokenizer.tokenize(sample['question'])\n",
        "\n",
        "        if len(query_tokens) > max_query_length:\n",
        "            query_tokens = query_tokens[0:max_query_length]\n",
        "\n",
        "        tok_to_orig_index = []\n",
        "        orig_to_tok_index = []\n",
        "        all_doc_tokens = []\n",
        "        for (i, token) in enumerate(sample['context']):\n",
        "            orig_to_tok_index.append(len(all_doc_tokens))\n",
        "            sub_tokens = tokenizer.tokenize(token)\n",
        "            for sub_token in sub_tokens:\n",
        "                tok_to_orig_index.append(i)\n",
        "                all_doc_tokens.append(sub_token)\n",
        "\n",
        "        tok_start_position = None\n",
        "        tok_end_position = None\n",
        "\n",
        "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
        "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
        "\n",
        "        # We can have documents that are longer than the maximum sequence length.\n",
        "        # To deal with this we do a sliding window approach, where we take chunks\n",
        "        # of the up to our max length with a stride of `doc_stride`.\n",
        "\n",
        "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "            \"DocSpan\", [\"start\", \"length\"])\n",
        "        doc_spans = []\n",
        "        start_offset = 0\n",
        "        while start_offset < len(all_doc_tokens):\n",
        "            length = len(all_doc_tokens) - start_offset\n",
        "            if length > max_tokens_for_doc:\n",
        "                length = max_tokens_for_doc\n",
        "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "            if start_offset + length == len(all_doc_tokens):\n",
        "                break\n",
        "            start_offset += min(length, doc_stride)\n",
        "\n",
        "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
        "            tokens = []\n",
        "            token_to_orig_map = {}\n",
        "            token_is_max_context = {}\n",
        "            segment_ids = []\n",
        "            tokens.append(\"[CLS]\")\n",
        "            segment_ids.append(0)\n",
        "            for token in query_tokens:\n",
        "                tokens.append(token)\n",
        "                segment_ids.append(0)\n",
        "            tokens.append(\"[SEP]\")\n",
        "            segment_ids.append(0)\n",
        "\n",
        "            for i in range(doc_span.length):\n",
        "                split_token_index = doc_span.start + i\n",
        "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
        "\n",
        "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
        "                                                       split_token_index)\n",
        "                token_is_max_context[len(tokens)] = is_max_context\n",
        "                tokens.append(all_doc_tokens[split_token_index])\n",
        "                segment_ids.append(1)\n",
        "            tokens.append(\"[SEP]\")\n",
        "            segment_ids.append(1)\n",
        "\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "            # tokens are attended to.\n",
        "            input_mask = [1] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.\n",
        "            while len(input_ids) < max_seq_length:\n",
        "                input_ids.append(0)\n",
        "                input_mask.append(0)\n",
        "                segment_ids.append(0)\n",
        "\n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segment_ids) == max_seq_length\n",
        "\n",
        "            start_position = None\n",
        "            end_position = None\n",
        "\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"unique_id: %s\" % (unique_id))\n",
        "            logger.info(\"sample_index: %s\" % (sample_index))\n",
        "            logger.info(\"doc_span_index: %s\" % (doc_span_index))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                [printable_text(x) for x in tokens]))\n",
        "            logger.info(\"token_to_orig_map: %s\" % \" \".join([\n",
        "                \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()]))\n",
        "            logger.info(\"token_is_max_context: %s\" % \" \".join([\n",
        "                \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n",
        "            ]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\n",
        "                \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\n",
        "                \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "\n",
        "            features.append(\n",
        "                InputFeatures(\n",
        "                    unique_id=unique_id,\n",
        "                    sample_index=sample_index,\n",
        "                    doc_span_index=doc_span_index,\n",
        "                    tokens=tokens,\n",
        "                    token_to_orig_map=token_to_orig_map,\n",
        "                    token_is_max_context=token_is_max_context,\n",
        "                    input_ids=input_ids,\n",
        "                    input_mask=input_mask,\n",
        "                    segment_ids=segment_ids,\n",
        "                    start_position=start_position,\n",
        "                    end_position=end_position))\n",
        "            unique_id += 1\n",
        "\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rUIN9oiPL7ON",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_sample(contextfile, question):\n",
        "    context = []\n",
        "    with open (contextfile) as context_file:\n",
        "        context.extend(context_file.readline().split(\" \"))\n",
        "    return {\"context\": context, \"question\": question}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xaWRQypOL7OS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_predictions(model, tokenizer, device, output_file):\n",
        "#     contextfile = \"parsed-20160719-DODAC-Transcript.txt\"\n",
        "    contextfile = \"test-input.txt\"\n",
        "    question = \"Which Carolina Panthers player was named Most Valuable Player?\"\n",
        "    \n",
        "    eval_samples = [load_sample(contextfile, question)] # wrap in list\n",
        "    eval_features = convert_samples_to_features(\n",
        "            samples=eval_samples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=MAX_SEQ_LENGTH,\n",
        "            doc_stride=DOC_STRIDE,\n",
        "            max_query_length=MAX_QUERY_LENGTH,\n",
        "            is_training=False)\n",
        "\n",
        "    logger.info(\"***** Running predictions *****\")\n",
        "    logger.info(\"  Num orig examples = %d\", len(eval_samples))\n",
        "    logger.info(\"  Num split examples = %d\", len(eval_features))\n",
        "\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
        "    all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
        "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n",
        "    \n",
        "    if True: # args.local_rank == -1\n",
        "        eval_sampler = SequentialSampler(eval_data)\n",
        "    else:\n",
        "        eval_sampler = DistributedSampler(eval_data)\n",
        "        \n",
        "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=PREDICT_BATCH_SIZE)\n",
        "\n",
        "    model.eval()\n",
        "    all_results = []\n",
        "    logger.info(\"Start evaluating\")\n",
        "    for input_ids, input_mask, segment_ids, example_indices in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        if len(all_results) % 1000 == 0:\n",
        "            logger.info(\"Processing example: %d\" % (len(all_results)))\n",
        "        input_ids = input_ids.to(device)\n",
        "        input_mask = input_mask.to(device)\n",
        "        segment_ids = segment_ids.to(device)\n",
        "        with torch.no_grad():\n",
        "            batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n",
        "        for i, example_index in enumerate(example_indices):\n",
        "            start_logits = batch_start_logits[i].detach().cpu().tolist()\n",
        "            end_logits = batch_end_logits[i].detach().cpu().tolist()\n",
        "            eval_feature = eval_features[example_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "            all_results.append(RawResult(unique_id=unique_id,\n",
        "                                            start_logits=start_logits,\n",
        "                                            end_logits=end_logits))\n",
        "    return eval_samples, eval_features, all_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GzLsnmtRL7OV",
        "colab_type": "code",
        "outputId": "40b60f12-be8c-42c6-a7e7-a0ae7b858869",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bert_model = \"bert-large-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
        "\n",
        "# Set device to CUDA if available, cpu otherwise\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Prepare model\n",
        "model = BertForQuestionAnswering.from_pretrained(bert_model)\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/28/2018 15:39:49 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at /Users/jackpertschuk/.pytorch_pretrained_bert/9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "11/28/2018 15:39:49 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz from cache at /Users/jackpertschuk/.pytorch_pretrained_bert/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05\n",
            "11/28/2018 15:39:49 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/jackpertschuk/.pytorch_pretrained_bert/214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05 to temp dir /var/folders/ny/0y8lms6n01x8jbw6bxtj42gc0000gn/T/tmpu8a0iwm5\n",
            "11/28/2018 15:40:33 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "11/28/2018 15:41:01 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "11/28/2018 15:41:01 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 1024)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (12): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (13): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (14): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (15): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (16): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (17): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (18): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (19): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (20): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (21): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (22): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (23): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "xsWVxh7kL7Oa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=5e-5,\n",
        "                     warmup=0.1,\n",
        "                     t_total=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y3xRw-mmL7Oc",
        "colab_type": "code",
        "outputId": "daa367f2-fbf8-4fd6-c177-45d8983be56c",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_samples, eval_features, all_results = get_predictions(model, tokenizer, device, output_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'output_file' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-5ec4469770f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output_file' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "9gS7sbOsL7Of",
        "colab_type": "code",
        "outputId": "5feaea9c-4032-449e-e990-098f4c7cb611",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(all_results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[RawResult(unique_id=1000000000, start_logits=[-0.29533618688583374, -0.5295275449752808, 0.06320291012525558, 0.035421304404735565, 0.24979905784130096, -0.04186653345823288, -0.38094231486320496, -0.43459850549697876, -0.32280832529067993, 0.21196065843105316, -0.16321392357349396, -0.21736615896224976, -0.13246923685073853, -0.04411831870675087, -0.3765515089035034, 0.8015763163566589, -0.2547002136707306, -0.18818604946136475, -0.41425424814224243, 0.14365997910499573, -0.4825842082500458, -0.040614232420921326, -0.05480096489191055, -0.593811571598053, 0.0214245468378067, -0.4057045578956604, 0.1430887132883072, 0.14335379004478455, 0.053803570568561554, -0.2592712640762329, -0.12112627178430557, -0.2032630443572998, -0.47860389947891235, -0.12115390598773956, -0.0435342863202095, -0.22136057913303375, -0.32299861311912537, -0.14349332451820374, -0.07641804963350296, -0.44427886605262756, 0.8015563488006592, 0.8015871047973633, -0.04011714458465576, -0.6809523105621338, -0.13839799165725708, -0.2990418076515198, -0.08161121606826782, 0.00776151567697525, -0.03754975274205208, -0.03402813524007797, -0.6277796626091003, -0.3547947406768799, 0.1433199942111969, -0.6344385147094727, -0.09783115237951279, -0.12144151329994202, 0.07775597274303436, 0.2160869836807251, 0.14323808252811432, -0.6863588094711304, 0.14328645169734955, -0.5181676149368286, -0.5875537395477295, -0.05505944788455963, -0.15921933948993683, -0.624420166015625, -0.7654203772544861, -0.15098358690738678, -0.009938821196556091, -0.5232675075531006, -0.5073574781417847, -0.22106507420539856, -0.42926809191703796, 0.801594078540802, -0.04912063479423523, -0.24676844477653503, -0.38032853603363037, -0.20616215467453003, -0.24479296803474426, -0.22138771414756775, -0.4392496347427368, -0.7943370938301086, -0.357468843460083, 0.01765025407075882, -0.09509498625993729, -0.6175484657287598, 0.17576846480369568, -0.4690985679626465, 0.14290663599967957, 0.142885223031044, -0.45355236530303955, -0.10934923589229584, 0.8015612363815308, 0.3553895652294159, 0.15211422741413116, -0.3052217364311218, -0.42587631940841675, 0.14313846826553345, -0.3895515203475952, -0.20101824402809143, -0.3280285894870758, -0.475661039352417, -0.03476950526237488, -0.10099402070045471, -0.34581881761550903, 0.14304909110069275, -0.6773889660835266, -0.6584697961807251, 0.035169072449207306, -0.29462021589279175, 0.026350386440753937, -0.010672077536582947, -0.6345429420471191, -0.2518490254878998, -0.2320629060268402, -0.6429041624069214, -0.11746823787689209, -0.46878522634506226, -0.06282594799995422, 0.20237183570861816, 0.7684510946273804, -0.08527126908302307, -0.18067516386508942, -0.08231984078884125, -0.026120059192180634, 0.06990048289299011, 0.1433749496936798, -0.5522527098655701, -0.2411787360906601, -0.5875976085662842, -0.19152694940567017, -0.44787296652793884, 0.05295027792453766, 0.1431850790977478, 0.14316785335540771, -0.16301672160625458, -0.03760614991188049, 0.1433388888835907, -0.40412238240242004, -0.7155486345291138, -1.0460624694824219, -0.771466851234436, -0.6572719216346741, -0.19373458623886108, -0.06780704855918884, -0.1097390204668045, -0.5525826811790466, -0.24680134654045105, -0.033466748893260956, 0.006326984614133835, 0.07178633660078049, -0.32955098152160645, -0.02220124937593937, -0.32586029171943665, 0.1654871702194214, 0.03661166504025459, -0.161376953125, 0.034710824489593506, 0.11537151038646698, 0.06266585737466812, 0.0691361278295517, 0.07622341811656952, 0.3455563187599182, 0.36740970611572266, 0.28469163179397583, 0.2804974913597107, 0.22647079825401306, 0.047504376620054245, 0.3193831145763397, 0.2867887616157532, 0.052585355937480927, -0.0101962611079216, -0.06772314012050629, -0.0715743899345398, -0.07523969560861588, -0.168466717004776, -0.07098615169525146, -0.06102656573057175, -0.053660787642002106, -0.06825177371501923, -0.05162260681390762, -0.09889917075634003, -0.3596861958503723, 0.005661420524120331, -0.02376401424407959, -0.44402068853378296, 0.022302348166704178, 0.513361394405365, 0.10283935815095901, 0.09467621147632599, 0.10594089329242706, 0.1113351508975029, 0.028181910514831543, 0.017230063676834106, -0.020807959139347076, -0.01747886836528778, -0.03048144280910492, 0.005594070069491863, -0.01721816137433052, -0.02811254933476448, -0.02547401189804077, -0.06153412535786629, -0.036099374294281006, -0.014221467077732086, 0.23863956332206726, 0.0510515421628952, -0.008921079337596893, -0.006969928741455078, -0.005373336374759674, 0.029413938522338867, -0.027872934937477112, -0.0611647330224514, -0.08767328411340714, -0.06279979646205902, -0.05190552771091461, -0.11397981643676758, -0.04589882493019104, -0.05260971933603287, -0.01550397276878357, 0.012942194938659668, -0.03263748437166214, -0.05463652312755585, -0.053758688271045685, -0.05833866074681282, -0.09432151168584824, 0.045464642345905304, 0.05902143567800522, 0.009944912046194077, 0.012977682054042816, 0.037603363394737244, 0.016389697790145874, 0.0574989840388298, 0.025197293609380722, 0.023654192686080933, 0.01926000416278839, 0.007503911852836609, -0.01619282364845276, -0.014988049864768982, -0.051363036036491394, -0.03663748502731323, -0.03916827589273453, -0.034547314047813416, -0.048660531640052795, -0.04373110085725784, -0.05952685326337814, -0.04410312697291374, -0.06530953198671341, -0.07713871449232101, -0.08897579461336136, -0.07891552150249481, -0.2156040072441101, -0.10054399818181992, -0.057172734290361404, -0.10619624704122543, -0.09568492323160172, -0.03470683842897415, -0.17083653807640076, 0.09476856142282486, 0.35826876759529114, 0.022067159414291382, 0.016534462571144104, 0.13472697138786316, 0.037546299397945404, 0.014031324535608292, -0.38329654932022095, 0.0397745780646801, -0.3078809380531311, -0.18244411051273346, -0.016170348972082138, -0.03626050055027008, -0.04492855444550514, -0.04906502366065979, -5.5421143770217896e-05, 0.1000404804944992, -0.018206343054771423, 0.42793941497802734, -0.06644546985626221, -0.045282747596502304, -0.31442517042160034, -0.10409683734178543, -0.6321596503257751, -0.23189495503902435, -0.26640692353248596, -0.026147663593292236, -0.024738527834415436, -0.19324703514575958, 0.1274053156375885, -0.3375151753425598, 0.3497311472892761, -0.2262907326221466, -0.05756658315658569, -0.022441506385803223, -0.03868703916668892, -0.4701092839241028, 0.05134551227092743, -0.06860385835170746, 0.2457827627658844, 0.030879415571689606, -0.06731149554252625, 0.12014459073543549, -0.05420074611902237, 0.1163274347782135, -0.3220953345298767, -0.11582773923873901, -0.23512527346611023, -0.05305436998605728, -0.1356852948665619, -0.12195426970720291, -0.12758098542690277, -0.07877595722675323, -0.1632101982831955, -0.17449967563152313, -0.0387294664978981, -0.34198856353759766, -0.045353151857852936, -0.7202951312065125, -0.41611307859420776, -0.254833459854126, -0.2695157825946808, -0.4434569180011749, -0.3204582631587982, -0.43507957458496094, -0.30295154452323914, -0.27782580256462097, 0.04131423681974411, -0.007150381803512573, -0.004251979291439056, -0.16979102790355682, 0.04696311056613922, 0.13645713031291962, 0.02324284240603447, 0.07760149240493774, -0.18807151913642883, -0.19065487384796143, -0.34958624839782715, -0.2550773322582245, 0.0290464386343956, 0.2811703085899353, 0.11326843500137329, 0.325278103351593, -0.13436485826969147, -0.01586931198835373, -0.8294317722320557, -0.2905253469944, -0.3507205545902252, -0.21635285019874573, -0.27460798621177673, -0.04783368855714798, -0.20086634159088135, -0.17374002933502197, -0.3167617917060852, -0.3013054430484772, -0.6125869750976562, -0.2884758412837982, -0.12937843799591064, -0.23492711782455444, -0.13643717765808105, -0.05022826045751572, -0.08419211208820343, -0.2876882553100586, -0.2880875766277313, -0.2383296936750412, -0.23224371671676636, -0.006173569709062576, 0.04683120176196098, 0.04344334080815315, -0.09428593516349792, -0.22341075539588928, -0.029980290681123734, 0.010229076258838177, -0.49643880128860474, -0.5047751665115356, -0.3452654480934143, -0.7673035860061646, -0.3865857720375061, -0.15587188303470612, -0.10207152366638184, -0.11939211189746857, -0.156840518116951, -0.11782550811767578, -0.15752509236335754, -0.0746987909078598, -0.13421207666397095, 0.004233658313751221], end_logits=[0.6234814524650574, 0.02260061353445053, -0.1292676329612732, -0.2518731355667114, -0.36598291993141174, 0.10687150806188583, 0.020638056099414825, 0.18025347590446472, -0.05098329484462738, -0.3870141804218292, -0.5036609768867493, -0.21873268485069275, -0.1908775269985199, -0.40189433097839355, 0.07671603560447693, 0.8943566679954529, -0.08245205879211426, -0.47008001804351807, 0.058226995170116425, 0.8253403902053833, 0.030659057199954987, -0.08453139662742615, -0.029645271599292755, -0.40589234232902527, -0.7297652363777161, -0.3775818943977356, 0.8251169919967651, 0.8252049684524536, -0.5818445086479187, -0.5764392614364624, -0.6087111234664917, -0.21213476359844208, -0.009068801999092102, -0.14466193318367004, -0.2769645154476166, 0.041719481348991394, -0.10208186507225037, -0.4807935953140259, -0.33609047532081604, 0.11061403155326843, 0.8942722678184509, 0.894355833530426, -0.6327981948852539, -0.016969187185168266, -0.2599453926086426, -0.7175596952438354, -0.47401466965675354, -0.45184630155563354, -0.00950593687593937, -0.06481245160102844, -0.3307664394378662, -0.7575376033782959, 0.8254817724227905, -0.31941652297973633, -0.2501178979873657, -0.33958700299263, -0.40101054310798645, -0.3693235516548157, 0.8250056505203247, -0.4203769564628601, 0.825605571269989, -0.6480361223220825, 0.17262595891952515, -0.24581755697727203, -0.32723551988601685, -0.32805174589157104, -0.27511122822761536, -0.28626662492752075, -0.5864425301551819, -0.12638258934020996, -0.10674571990966797, -0.1545606404542923, -0.3488960266113281, 0.894839882850647, -0.233179971575737, -0.5457720756530762, -0.009503882378339767, -0.25941362977027893, -0.07141181826591492, -0.4468305706977844, 0.09819217771291733, 0.19786150753498077, -0.26066505908966064, -0.026163408532738686, 0.014282934367656708, -0.4432573914527893, -0.4728759229183197, -0.3025722801685333, 0.8248704075813293, 0.8249959945678711, -0.07825902104377747, -0.2659382224082947, 0.8942880630493164, -0.581048846244812, -0.5056486129760742, -0.36030256748199463, -0.3025663495063782, 0.8249108195304871, -0.3366859257221222, -0.7940545678138733, -0.6756511330604553, -0.7104359269142151, -0.18276380002498627, -0.515471339225769, -0.5145854949951172, 0.8253591060638428, -0.4390779435634613, 0.05945692956447601, -0.6815257668495178, 0.030189525336027145, -0.05324177071452141, 0.014048401266336441, -0.27806204557418823, -0.5900574922561646, -0.45196396112442017, -0.42282170057296753, -0.2609138488769531, -0.6015761494636536, -0.47020673751831055, -0.45078375935554504, 0.6668740510940552, -0.8080813884735107, -0.079984650015831, -0.2187524437904358, -0.31761273741722107, -0.746740460395813, 0.8253465294837952, -0.22906692326068878, -0.39798465371131897, -0.18431250751018524, -0.7148377299308777, -0.48246055841445923, -0.12639710307121277, 0.8253449201583862, 0.8253191709518433, -0.23246389627456665, -0.7971853017807007, 0.8251867294311523, -0.24352498352527618, -0.14191539585590363, -0.3608878254890442, -0.21525222063064575, -0.3323092460632324, -0.20172154903411865, -0.026489093899726868, -0.3405000567436218, -0.04018700122833252, -0.36497896909713745, -0.19877049326896667, -0.2783750295639038, -0.21800273656845093, -0.02105274423956871, -0.03754367679357529, -0.3956313729286194, -0.38112199306488037, -0.3367586135864258, -0.3670254647731781, -0.14319629967212677, -0.2705857455730438, -0.27119845151901245, -0.2751615643501282, -0.2569328546524048, -0.25198426842689514, -0.19016414880752563, -0.2065207064151764, -0.2471475899219513, 0.004974260926246643, -0.00982828438282013, 0.0014409273862838745, -0.10948693752288818, -0.21945615112781525, -0.2711160480976105, -0.2826229929924011, -0.27036693692207336, -0.27934375405311584, -0.32292407751083374, -0.2634863555431366, -0.26761165261268616, -0.2864169776439667, -0.2767961919307709, -0.2562847137451172, -0.17404770851135254, -0.10427992045879364, -0.1577361822128296, -0.26659446954727173, -0.03382185101509094, -0.2573363184928894, -0.2976880967617035, -0.20509228110313416, -0.00013747811317443848, -0.03935079276561737, -0.22683648765087128, -0.25735098123550415, -0.27022087574005127, -0.31058257818222046, -0.26723623275756836, -0.2697674632072449, -0.16436995565891266, -0.2966653108596802, -0.2859402000904083, -0.289264976978302, -0.3143671154975891, -0.2699856758117676, -0.2481667697429657, -0.2637154161930084, -0.19855861365795135, -0.22069816291332245, -0.22682932019233704, -0.22579988837242126, -0.22741398215293884, -0.2554703950881958, -0.2879604995250702, -0.27992087602615356, -0.25561752915382385, -0.2508396506309509, -0.27375924587249756, -0.25040203332901, -0.2570251524448395, -0.26587986946105957, -0.25093522667884827, -0.2501406967639923, -0.3024941384792328, -0.28291255235671997, -0.28144702315330505, -0.18548882007598877, -0.2617398798465729, -0.303241491317749, -0.3196229338645935, -0.33108067512512207, -0.3142926096916199, -0.28227195143699646, -0.2313409149646759, 0.04852675274014473, -0.23265016078948975, -0.22775477170944214, -0.22616809606552124, -0.25755420327186584, -0.2467932552099228, -0.27724573016166687, -0.25679659843444824, -0.21940991282463074, -0.2669088542461395, -0.2767670750617981, -0.21288961172103882, -0.22575643658638, -0.25026804208755493, -0.2394767701625824, -0.23955640196800232, -0.26021307706832886, -0.22592905163764954, -0.2140251100063324, -0.23925085365772247, -0.32958459854125977, -0.2367706596851349, -0.2914658784866333, -0.3053097128868103, -0.22399690747261047, -0.289020299911499, -0.40697839856147766, -0.2357926070690155, -0.24503646790981293, -0.24122092127799988, -0.25565221905708313, -0.2747655510902405, 0.10843759775161743, -0.29716119170188904, -0.04123843088746071, -0.2569916844367981, -0.3441128432750702, -0.31974244117736816, -0.28771406412124634, -0.2880336344242096, -0.8707208633422852, 0.06557688117027283, -0.21647173166275024, -0.31387829780578613, -0.23630641400814056, -0.2531348466873169, -0.45973140001296997, -0.23824644088745117, -0.46411699056625366, -0.5851948261260986, -0.5416812300682068, -0.6098169088363647, -0.753311276435852, -0.19302698969841003, -0.3687998354434967, -0.36008089780807495, -0.17599067091941833, -0.014801621437072754, -0.26889997720718384, -0.2903378903865814, -0.2228119671344757, -0.40681493282318115, -0.6227485537528992, -0.6030048131942749, -0.3118961751461029, -0.29647672176361084, -0.3253987431526184, 0.009373974055051804, -0.1603989452123642, 0.16079860925674438, 0.06628815829753876, -0.038554638624191284, 0.0034090206027030945, -0.36161184310913086, -0.4002128541469574, -0.4186249077320099, -0.3868453800678253, -0.7053695917129517, -0.6236362457275391, -0.24145400524139404, -0.2297549843788147, -0.27098900079727173, -0.2255808711051941, -0.5467727780342102, -0.6407577991485596, -0.2858569025993347, -0.20997045934200287, -0.04657623916864395, -0.24962183833122253, -0.5369621515274048, -0.3334500789642334, -0.5140593647956848, -0.6250209808349609, -0.6250840425491333, -0.36490398645401, -0.3146200478076935, -0.005330078303813934, -0.017484664916992188, -0.10303761065006256, 0.19048956036567688, 0.14862950146198273, -0.16474959254264832, -0.002102896571159363, -0.3035697638988495, -0.5948076844215393, -0.039110198616981506, -0.32095766067504883, -0.6558402180671692, -0.4519437849521637, -0.247531458735466, -0.5181503295898438, -0.46015334129333496, -0.731044352054596, -0.5489503145217896, -0.4279957413673401, -0.30199116468429565, -0.40458932518959045, -0.32216542959213257, -0.42897462844848633, -0.38664358854293823, -0.5909364223480225, -0.303472101688385, -0.2843823730945587, -0.3300740122795105, -0.4302929639816284, -0.5872495770454407, -0.3660411536693573, -0.574648380279541, -0.37976187467575073, -0.29082414507865906, -0.3539476990699768, -0.5100349187850952, -0.20859715342521667, -0.2610001564025879, -0.5413434505462646, -0.3317191004753113, -0.24492385983467102, -0.33409857749938965, -0.5326594710350037, -0.041493698954582214, -0.4499627649784088, -0.21947062015533447, -0.29067644476890564, -0.5352343320846558, -0.21610352396965027, -0.2630629539489746, -0.2522928714752197, -0.24297119677066803, -0.2724941372871399, -0.2375601828098297, -0.25523847341537476, -0.3606555461883545])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TbUWjWVgL7Oi",
        "colab_type": "code",
        "outputId": "51746cdd-71ca-4610-a1b8-33b9a6f4766c",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output_prediction_file = \"predictions_output.txt\"\n",
        "\n",
        "nbest_json = write_predictions(eval_samples, eval_features, all_results,\n",
        "                    output_prediction_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/26/2018 22:28:42 - INFO - __main__ -   Writing predictions to: predictions_output.txt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "wywq4LAaL7On",
        "colab_type": "code",
        "outputId": "d4068816-cae4-4a2c-9a3c-5a4be24ce4e8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(nbest_json)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[OrderedDict([('text', '49'), ('probability', 0.22005406039708847), ('start_logit', 0.6856483817100525), ('end_logit', 0.7917672395706177)]), OrderedDict([('text', '49\\\\u201315 in the NFC Championship Game and advanced to their second Super Bowl appearance since the franchise was founded in 1995. The Broncos finished the regular season with a 12\\\\u20134 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 20'), ('probability', 0.2124478382653197), ('start_logit', 0.6856483817100525), ('end_logit', 0.7565904855728149)]), OrderedDict([('text', '49\\\\u201315 in the NFC Championship Game and advanced to their second Super Bowl appearance since the franchise was founded in 1995. The Broncos finished the regular season with a 12\\\\u20134 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 20\\\\u201318 in the AFC Championship Game. They joined the Patriots,'), ('probability', 0.20019751540472758), ('start_logit', 0.6856483817100525), ('end_logit', 0.6971984505653381)]), OrderedDict([('text', 'Valuable Player (MVP). They defeated the Arizona Cardinals 49'), ('probability', 0.18551547528488643), ('start_logit', 0.5149134397506714), ('end_logit', 0.7917672395706177)]), OrderedDict([('text', 'Broncos finished the regular season with a 12\\\\u20134 record, and denied the New England Patriots a chance to defend their title from Super Bowl XLIX by defeating them 20'), ('probability', 0.18178511064797787), ('start_logit', 0.5297771692276001), ('end_logit', 0.7565904855728149)])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ditOZ4yoL7Os",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}